{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70129035-6fc3-4809-a629-e4efb5e80d69",
   "metadata": {},
   "source": [
    "1.Lasso Regression:\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) Regression is a type of linear regression that includes an L1 penalty on the magnitude of the coefficients. This penalty term adds the absolute value of the coefficients to the loss function.\n",
    "\n",
    "Formula:\n",
    "The Lasso Regression loss function is:\n",
    "\n",
    "Loss\n",
    "=\n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "(\n",
    "𝑦\n",
    "𝑖\n",
    "−\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    ")\n",
    "2\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "∣\n",
    "𝛽\n",
    "𝑗\n",
    "∣\n",
    "Loss=∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " (y \n",
    "i\n",
    "​\n",
    " − \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    " ) \n",
    "2\n",
    " +λ∑ \n",
    "j=1\n",
    "p\n",
    "​\n",
    " ∣β \n",
    "j\n",
    "​\n",
    " ∣\n",
    "\n",
    "Where:\n",
    "\n",
    "𝑦\n",
    "𝑖\n",
    "y \n",
    "i\n",
    "​\n",
    "  are the actual values.\n",
    "𝑦\n",
    "^\n",
    "𝑖\n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "i\n",
    "​\n",
    "  are the predicted values.\n",
    "𝛽\n",
    "𝑗\n",
    "β \n",
    "j\n",
    "​\n",
    "  are the coefficients.\n",
    "𝜆\n",
    "λ (lambda) is the regularization parameter.\n",
    "Differences from Other Regression Techniques:\n",
    "\n",
    "Ordinary Least Squares (OLS) Regression: OLS minimizes the sum of squared residuals without any regularization, potentially leading to overfitting in the presence of many predictors.\n",
    "Ridge Regression: Adds an L2 penalty (squared coefficients) instead of an L1 penalty, leading to shrinkage but not zeroing out coefficients.\n",
    "Elastic Net: Combines both L1 and L2 penalties to balance between Lasso and Ridge properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04538bff-3ec0-44ea-9e9f-4c4894e7c439",
   "metadata": {},
   "source": [
    "2.Feature Selection Advantage:\n",
    "Lasso Regression can automatically select features by shrinking some coefficients to exactly zero. This makes it useful for models with many predictors, helping to identify the most relevant features and reducing model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d63ca83-0d3c-432a-812b-55956912f5cb",
   "metadata": {},
   "source": [
    "3.Interpreting Coefficients:\n",
    "\n",
    "Magnitude and Sign: The sign of the coefficients indicates the direction of the relationship (positive or negative) between the predictor and the response variable.\n",
    "Zero Coefficients: Predictors with coefficients exactly equal to zero are not used in the model, indicating that Lasso has excluded them as irrelevant.\n",
    "Non-zero Coefficients: The magnitude of non-zero coefficients indicates the strength of the relationship, though the values are shrunk due to the regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cad3b-a73b-431d-bb30-de61821b01f7",
   "metadata": {},
   "source": [
    "4.Main Tuning Parameter:\n",
    "\n",
    "Lambda (\n",
    "𝜆\n",
    "λ): Controls the strength of the penalty.\n",
    "Effects:\n",
    "\n",
    "High Lambda: Strong regularization, more coefficients shrunk to zero, potentially underfitting.\n",
    "Low Lambda: Weak regularization, coefficients closer to those from OLS, potentially overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e58936-e69e-46f8-b52e-da019150125b",
   "metadata": {},
   "source": [
    "5.Using Lasso for Non-linear Problems:\n",
    "\n",
    "Feature Engineering: Transform the input features to capture non-linear relationships, such as polynomial features or interaction terms.\n",
    "Example: Create polynomial features of the original predictors and then apply Lasso Regression to select among these transformed features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d1afa-a3d4-485c-be1c-a33af660798e",
   "metadata": {},
   "source": [
    "6.Key Differences:\n",
    "\n",
    "Penalty Type:\n",
    "Ridge Regression: Uses L2 penalty (squared coefficients), leading to shrinkage but no zero coefficients.\n",
    "Lasso Regression: Uses L1 penalty (absolute value of coefficients), leading to some coefficients being exactly zero, facilitating feature selection.\n",
    "Feature Selection:\n",
    "Ridge: Does not perform feature selection, all features are retained.\n",
    "Lasso: Performs feature selection by shrinking some coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc88f9-07fc-485a-ae29-09801eb57107",
   "metadata": {},
   "source": [
    "7.Handling Multicollinearity:\n",
    "\n",
    "Yes: Lasso Regression can handle multicollinearity by shrinking the coefficients of correlated predictors, often reducing some to zero. This results in selecting one predictor from a group of highly correlated predictors, effectively dealing with multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dac791-11ea-4928-8aec-2338a9bd6c2c",
   "metadata": {},
   "source": [
    "8.Methods to Choose Lambda:\n",
    "\n",
    "Cross-Validation: Perform k-fold cross-validation to select the lambda that minimizes the cross-validation error.\n",
    "Grid Search: Test a range of lambda values and choose the one with the best performance based on a chosen metric (e.g., RMSE).\n",
    "Regularization Path: Plot the coefficient paths as a function of lambda and select a value that balances bias and variance.\n",
    "By understanding and applying these principles and techniques, you can effectively use Lasso Regression to build robust, interpretable models, particularly in situations with many predictors and potential multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9a725-60e7-46df-92a0-901c281e667b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
